---
title: "H2O AutoML Model"
author: ""
date: "`r format(lubridate::today(), '%d %b %Y')`"
output: html_document

params:
  dpath: !R NULL
  target_name: !R target
  outpath: !R assets
  name: "h2o"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# find the root directory for the project and set that as wd
root_dir <- rprojroot::find_root(rprojroot::is_rstudio_project)
knitr::opts_knit$set(root.dir = root_dir)
```


## Overview

The notebook aims to:

1. Train a h2o model
2. Evaluate model performance
3. Save the model object as asset


## Load Libraries

```{r load, message=FALSE, warning=FALSE}
library(skimr)          # summary stats
library(tidyverse)      # data manipulation
library(tidymodels)     # modelling
library(highcharter)    # interactive visualization
library(janitor)        # clean column names, convert to snake case
library(h2o)            # for automl
```

## Initialisation

Start from specifying cluster, with a clean slate - just in case the cluster was already running.

```{r init, message=FALSE, warning=FALSE}
h2o.init(nthreads=-1, max_mem_size="2G")
h2o.removeAll()
```


## Import Data 

```{r import, message=FALSE, warning=FALSE}
dat <- read_csv(params$dpath) %>% 
  rename(target = params$target_name) %>% 
  clean_names()
dim(dat)
```
 
 ## Preprocessing
 
- Convert variable class eg. make sure date is in correct format
- Variable validation
- Derive new features (make sure don't introduce leakage between train and test)


## Train and Test Split

```{r split}
set.seed(42)
dat_ls = h2o.splitFrame(dat, ratios=.75, destination_frames = c("train","test"))

dim(dat_ls[[1]])
dim(dat_ls[[2]])
```


## Model Fitting

```{r fit}
# Run AutoML for 10 base models (limited to 1 hour max runtime by default)
m1 <- h2o.automl(y = 'target',
                training_frame = data[[1]],
                max_models = 10,
                seed = 1)
m1@leader
```




## Evaluation

We calculate evaluation metrics on both train and test sets.

NB: If the model performs much better on train than on test, it is likely to be overfitting. 

### Metrics

```{r model_metrics, message=FALSE, warning=FALSE}
# train
results_train =  bind_cols(
  as.data.frame(data[[1]]$target), 
  as.data.frame(h2o.predict(m1, data[[1]]))
)
  
# test
results_test = bind_cols(
  as.data.frame(data[[2]]$target), 
  as.data.frame(h2o.predict(m1, data[[2]]))
)

# combine
list(results_train, results_test) %>% 
  map_dfc(~ yardstick::metrics(truth=target, estimate=predict, data=.x)) %>% 
  select(-contains('.estimator')) %>% 
  select(-.metric1) %>% 
  set_names('Metric', 'Train Score', 'Test Score')
```



### Comparison of actuals VS predictions

```{r actual_vs_pred, message=FALSE, warning=FALSE}
GnT::skplot_actual_vs_pred(data = results_test, 'target', yvar = 'predict')
```

## Save Model Object

```{r save, message=FALSE, warning=FALSE}
saveRDS(fit, file.path(params$outpath, str_c(params$name, ".rds")))
```

